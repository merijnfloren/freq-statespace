{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Wiener-Hammerstein system\n",
    "\n",
    "See http://arxiv.org/pdf/1708.06543 for more information on the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nonlinear_benchmarks as nlb\n",
    "import numpy as np\n",
    "import optimistix as optx\n",
    "\n",
    "from src import best_linear_approximation as bla\n",
    "from src import basis_functions, data_manager, inference_and_learning, nonlinear_functions\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)  # for reproducibility\n",
    "\n",
    "# Load data\n",
    "ParWH_full_train, ParWH_full_test = nlb.ParWH() \n",
    "\n",
    "# Initialise variables\n",
    "N = 16384  # number of samples per period\n",
    "R = 5  # number of random phase multisine realisations\n",
    "P = 2  # number of periods\n",
    "amplitude_level = 4  # must be one of {0, 1, 2, 3, 4}\n",
    "\n",
    "nu, ny = 1, 1  # SISO system\n",
    "\n",
    "fs = 78e3 # [Hz]\n",
    "f_idx = np.arange(1, 4096)  # frequency lines of interest (excludes DC)\n",
    "\n",
    "# Load data\n",
    "ParWH_full_train, ParWH_full_test = nlb.ParWH() \n",
    "ParWH_train = [\n",
    "    data for data in ParWH_full_train\n",
    "    for phase in range(R)\n",
    "    if data.name == f'Est-phase-{phase}-amp-{amplitude_level}'\n",
    "]\n",
    "ParWH_test = [\n",
    "    data for data in ParWH_full_test\n",
    "    if data.name == f'Val-amp-{amplitude_level}'\n",
    "][0]\n",
    "\n",
    "# Preprocess data\n",
    "u_train = np.array([data.u for data in ParWH_train]).reshape(R, nu, N, P)\n",
    "y_train = np.array([data.y for data in ParWH_train]).reshape(R, ny, N, P)\n",
    "u_train = np.transpose(u_train, (2, 1, 0, 3))\n",
    "y_train = np.transpose(y_train, (2, 1, 0, 3))\n",
    "\n",
    "u_test = np.transpose(ParWH_test.u.reshape(1, nu, N, 2), (2, 1, 0, 3))\n",
    "y_test = np.transpose(ParWH_test.y.reshape(1, ny, N, 2), (2, 1, 0, 3))\n",
    "\n",
    "# Create input-output training data object\n",
    "io_data = data_manager.create_data_object(u_train, y_train, f_idx, fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Best Linear Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSE of FSID BLA: 13.31%\n",
      "\n",
      "Starting iterative optimization...\n",
      "   Iteration 0, Loss: 2.1803e+00\n",
      "   Iteration 1, Loss: 1.7492e+00\n",
      "   Iteration 2, Loss: 1.7492e+00\n",
      "   Iteration 3, Loss: 1.7492e+00\n",
      "   Iteration 4, Loss: 1.6736e+00\n",
      "   Iteration 5, Loss: 1.6576e+00\n",
      "   Iteration 6, Loss: 1.5595e+00\n",
      "   Iteration 7, Loss: 1.5328e+00\n",
      "   Iteration 8, Loss: 1.5101e+00\n",
      "   Iteration 9, Loss: 1.4770e+00\n",
      "   Iteration 10, Loss: 1.4434e+00\n",
      "   Iteration 11, Loss: 1.4219e+00\n",
      "   Iteration 12, Loss: 1.4133e+00\n",
      "   Iteration 13, Loss: 1.4100e+00\n",
      "   Iteration 14, Loss: 1.4081e+00\n",
      "   Iteration 15, Loss: 1.4067e+00\n",
      "   Iteration 16, Loss: 1.4054e+00\n",
      "   Iteration 17, Loss: 1.4042e+00\n",
      "   Iteration 18, Loss: 1.4030e+00\n",
      "   Iteration 19, Loss: 1.4016e+00\n",
      "   Iteration 20, Loss: 1.4001e+00\n",
      "   Iteration 21, Loss: 1.3982e+00\n",
      "   Iteration 22, Loss: 1.3957e+00\n",
      "   Iteration 23, Loss: 1.3918e+00\n",
      "   Iteration 24, Loss: 1.3918e+00\n",
      "   Iteration 25, Loss: 1.3857e+00\n",
      "   Iteration 26, Loss: 1.3857e+00\n",
      "   Iteration 27, Loss: 1.3753e+00\n",
      "   Iteration 28, Loss: 1.3753e+00\n",
      "   Iteration 29, Loss: 1.3551e+00\n",
      "   Iteration 30, Loss: 1.3551e+00\n",
      "   Iteration 31, Loss: 1.3431e+00\n",
      "   Iteration 32, Loss: 1.3199e+00\n",
      "   Iteration 33, Loss: 1.3118e+00\n",
      "   Iteration 34, Loss: 1.3091e+00\n",
      "   Iteration 35, Loss: 1.3071e+00\n",
      "   Iteration 36, Loss: 1.3057e+00\n",
      "   Iteration 37, Loss: 1.3026e+00\n",
      "   Iteration 38, Loss: 1.3002e+00\n",
      "   Iteration 39, Loss: 1.2990e+00\n",
      "   Iteration 40, Loss: 1.2983e+00\n",
      "   Iteration 41, Loss: 1.2979e+00\n",
      "   Iteration 42, Loss: 1.2975e+00\n",
      "   Iteration 43, Loss: 1.2973e+00\n",
      "   Iteration 44, Loss: 1.2968e+00\n",
      "   Iteration 45, Loss: 1.2961e+00\n",
      "   Iteration 46, Loss: 1.2956e+00\n",
      "   Iteration 47, Loss: 1.2952e+00\n",
      "   Iteration 48, Loss: 1.2948e+00\n",
      "   Iteration 49, Loss: 1.2944e+00\n",
      "   Iteration 50, Loss: 1.2941e+00\n",
      "   Iteration 51, Loss: 1.2939e+00\n",
      "   Iteration 52, Loss: 1.2937e+00\n",
      "   Iteration 53, Loss: 1.2936e+00\n",
      "   Iteration 54, Loss: 1.2935e+00\n",
      "   Iteration 55, Loss: 1.2935e+00\n",
      "   Iteration 56, Loss: 1.2934e+00\n",
      "   Iteration 57, Loss: 1.2934e+00\n",
      "   Iteration 58, Loss: 1.2934e+00\n",
      "   Iteration 59, Loss: 1.2933e+00\n",
      "   Iteration 60, Loss: 1.2933e+00\n",
      "   Iteration 61, Loss: 1.2932e+00\n",
      "   Iteration 62, Loss: 1.2932e+00\n",
      "   Iteration 63, Loss: 1.2931e+00\n",
      "   Iteration 64, Loss: 1.2931e+00\n",
      "   Iteration 65, Loss: 1.2931e+00\n",
      "   Iteration 66, Loss: 1.2931e+00\n",
      "   Iteration 67, Loss: 1.2931e+00\n",
      "   Iteration 68, Loss: 1.2931e+00\n",
      "   Iteration 69, Loss: 1.2930e+00\n",
      "   Iteration 70, Loss: 1.2930e+00\n",
      "   Iteration 71, Loss: 1.2930e+00\n",
      "   Iteration 72, Loss: 1.2930e+00\n",
      "   Iteration 73, Loss: 1.2930e+00\n",
      "   Iteration 74, Loss: 1.2930e+00\n",
      "   Iteration 75, Loss: 1.2930e+00\n",
      "   Iteration 76, Loss: 1.2930e+00\n",
      "   Iteration 77, Loss: 1.2930e+00\n",
      "   Iteration 78, Loss: 1.2930e+00\n",
      "   Iteration 79, Loss: 1.2930e+00\n",
      "   Iteration 80, Loss: 1.2930e+00\n",
      "   Iteration 81, Loss: 1.2930e+00\n",
      "   Iteration 82, Loss: 1.2930e+00\n",
      "   Iteration 83, Loss: 1.2930e+00\n",
      "   Iteration 84, Loss: 1.2930e+00\n",
      "   Iteration 85, Loss: 1.2930e+00\n",
      "   Iteration 86, Loss: 1.2930e+00\n",
      "   Iteration 87, Loss: 1.2930e+00\n",
      "   Iteration 88, Loss: 1.2930e+00\n",
      "   Iteration 89, Loss: 1.2930e+00\n",
      "   Iteration 90, Loss: 1.2930e+00\n",
      "   Iteration 91, Loss: 1.2930e+00\n",
      "   Iteration 92, Loss: 1.2930e+00\n",
      "   Iteration 93, Loss: 1.2930e+00\n",
      "   Iteration 94, Loss: 1.2930e+00\n",
      "   Iteration 95, Loss: 1.2930e+00\n",
      "   Iteration 96, Loss: 1.2930e+00\n",
      "   Iteration 97, Loss: 1.2929e+00\n",
      "   Iteration 98, Loss: 1.2929e+00\n",
      "   Iteration 99, Loss: 1.2929e+00\n",
      "\n",
      "\n",
      "NRMSE of optimized BLA: 11.18%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### (i) Nonparametric estimate #####\n",
    "G_nonpar = bla.compute_nonparametric(io_data)\n",
    "\n",
    "##### (ii) Parametrize using frequency-domain subspace identification method #####\n",
    "nx = 12  # number of states\n",
    "q = nx + 1  # subspace dimensioning parameter\n",
    "bla_fsid = bla.freq_subspace_id(G_nonpar, nx, q)\n",
    "\n",
    "# Simulate and check time-domain performance\n",
    "u_bar = np.mean(io_data.time.u, axis=-1)  # we take the mean over the periods\n",
    "y_bar = np.mean(io_data.time.y, axis=-1)  # we take the mean over the periods\n",
    "handicap = 1000  # number of samples to start 'ahead of time' for transient effects to die out (only works for periodic data!)\n",
    "\n",
    "y_sim_bla_fsid = bla_fsid.simulate(u_bar, handicap=handicap)[0]\n",
    "NRMSE_bla_fsid = 100 * np.sqrt(np.mean((y_bar - y_sim_bla_fsid)**2)) / np.sqrt(np.mean(y_bar**2)) \n",
    "\n",
    "print(f'NRMSE of FSID BLA: {NRMSE_bla_fsid:.2f}%\\n')\n",
    "\n",
    "##### (iii) Frequency-domain iterative optimization starting from FSID BLA #####\n",
    "solver = optx.LevenbergMarquardt(rtol=1e-3, atol=1e-6)\n",
    "max_iter = 100\n",
    "\n",
    "bla_opti = bla.freq_iterative_optimization(G_nonpar, bla_fsid, solver, max_iter)\n",
    "\n",
    "# Simulate and check time-domain performance\n",
    "y_sim_bla_opti = bla_opti.simulate(u_bar, handicap=handicap)[0]\n",
    "NRMSE_bla_opti = 100 * np.sqrt(np.mean((y_bar - y_sim_bla_opti)**2)) / np.sqrt(np.mean(y_bar**2)) \n",
    "\n",
    "print(f'NRMSE of optimized BLA: {NRMSE_bla_opti:.2f}%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Union\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optimistix as optx\n",
    "\n",
    "from src.data_manager import FrequencyData, InputOutputData\n",
    "from src.basis_functions import AbstractBasisFunction\n",
    "from src.nonlinear_functions import create_custom_basis_function_model\n",
    "from src._model_structures import ModelBLA, ModelNonlinearLFR\n",
    "from src._solve import solve \n",
    "\n",
    "\n",
    "class _ThetaWZ(eqx.Module):\n",
    "    B_w_star: jnp.ndarray = eqx.field(converter=jnp.asarray)\n",
    "    C_z_star: jnp.ndarray = eqx.field(converter=jnp.asarray)\n",
    "    D_yw_star: jnp.ndarray = eqx.field(converter=jnp.asarray)\n",
    "    D_zu_star: jnp.ndarray = eqx.field(converter=jnp.asarray)\n",
    "\n",
    "\n",
    "class _OptiArgs(NamedTuple):\n",
    "    theta_uy: tuple\n",
    "    phi: AbstractBasisFunction\n",
    "    lambda_w: jnp.ndarray\n",
    "    fixed_point_iters: int\n",
    "    f_data: tuple\n",
    "    Lambda: jnp.ndarray\n",
    "    Tz_inv: jnp.ndarray\n",
    "    G_yu: jnp.ndarray\n",
    "    N: int\n",
    "\n",
    "\n",
    "def solverrr(\n",
    "   io_data: InputOutputData,\n",
    "   bla: ModelBLA,\n",
    "   phi: AbstractBasisFunction,\n",
    "   nw: int,\n",
    "   lambda_w: float,\n",
    "   fixed_point_iters: int,\n",
    "   solver: Union[optx.AbstractLeastSquaresSolver, optx.AbstractMinimiser],\n",
    "   max_iter: int,\n",
    "   seed: int\n",
    ") -> ModelNonlinearLFR:\n",
    "\n",
    "    theta0, args = _prepare_problem(\n",
    "        io_data, bla, phi, nw, lambda_w, fixed_point_iters, seed\n",
    "    )\n",
    "\n",
    "    # Optimize the model parameters\n",
    "    print('Starting iterative optimization...')\n",
    "    solve_result = solve(theta0, solver, args, _loss_fn, max_iter)\n",
    "    print('\\n')\n",
    "    \n",
    "    theta_opt = solve_result.theta\n",
    "    aux = solve_result.aux\n",
    "    \n",
    "    beta = aux[-1]\n",
    "    \n",
    "    return ModelNonlinearLFR(\n",
    "        A=args.theta_uy[0],\n",
    "        B_u=args.theta_uy[1],\n",
    "        C_y=args.theta_uy[2],\n",
    "        D_yu=bla.D_yu,\n",
    "        B_w=theta_opt.B_w_star,\n",
    "        C_z=args.Tz_inv @ theta_opt.C_z_star,\n",
    "        D_yw=theta_opt.D_yw_star,\n",
    "        D_zu=args.Tz_inv @ theta_opt.D_zu_star,\n",
    "        f_static=create_custom_basis_function_model(\n",
    "            nw, phi, beta\n",
    "        ),\n",
    "        ts=io_data.time.ts\n",
    "    )\n",
    "\n",
    "\n",
    "def _loss_fn(theta: _ThetaWZ, args: _OptiArgs) -> tuple:\\\n",
    "    \n",
    "    f_full, fs, U, Y, G_yu = args.f_data\n",
    "\n",
    "    A = args.theta_uy[0]\n",
    "    B_u = args.theta_uy[1]\n",
    "    C_y = args.theta_uy[2]\n",
    "\n",
    "    B_w = theta.B_w_star\n",
    "    C_z = args.Tz_inv @ theta.C_z_star\n",
    "    D_yw = theta.D_yw_star\n",
    "    D_zu = args.Tz_inv @ theta.D_zu_star\n",
    "\n",
    "    ny, nw = D_yw.shape\n",
    "    nz, nu = D_zu.shape\n",
    "    F = U.shape[0]\n",
    "    R = U.shape[2]\n",
    "\n",
    "    Theta = jnp.vstack((B_w, D_yw)).T @ jnp.vstack((B_w, D_yw))\n",
    "\n",
    "    z = 2 * jnp.pi * f_full / fs\n",
    "    zj = jnp.exp(z * 1j)\n",
    "\n",
    "    I_nw = jnp.eye(nw)\n",
    "    I_nx = jnp.eye(A.shape[0])\n",
    "\n",
    "    def _compute_parametric_Gs(k):\n",
    "        G_x = jnp.linalg.solve(zj[k] * I_nx - A, jnp.hstack((B_u, B_w)))\n",
    "        return (\n",
    "            C_y @ G_x[:, nu:] + D_yw,  # G_yw\n",
    "            C_z @ G_x[:, :nu] + D_zu,  # G_zu\n",
    "            C_z @ G_x[:, nu:]          # G_zw\n",
    "        )\n",
    "\n",
    "    G_yw, G_zu, G_zw = jax.vmap(_compute_parametric_Gs)(jnp.arange(F))\n",
    "\n",
    "    # --- Nonparametric inference ---\n",
    "    def _infer_nonparametric_signals(k):\n",
    "        Psi = G_yw[k, ...].T @ args.Lambda[k, ...]\n",
    "        Phi = Psi @ G_yw[k, ...] + args.lambda_w * Theta + 1e-10 * I_nw\n",
    "        W_hat = jnp.linalg.solve(\n",
    "            Phi,\n",
    "            Psi @ (Y[k, ...] - G_yu[k, ...] @ U[k, ...])\n",
    "        )\n",
    "        Z_hat = G_zu[k, ...] @ U[k, ...] + G_zw[k, ...] @ W_hat\n",
    "        Y_hat = G_yu[k, ...] @ U[k, ...] + G_yw[k, ...] @ W_hat\n",
    "        return W_hat, Z_hat, Y_hat\n",
    "\n",
    "    W_star, Z_star, Y_hat = jax.vmap(_infer_nonparametric_signals)(jnp.arange(F))  # noqa: E501\n",
    "\n",
    "    # --- Parametric learning ---\n",
    "    w_star = jnp.fft.irfft(W_star, n=args.N, axis=0)\n",
    "    z_star = jnp.fft.irfft(Z_star, n=args.N, axis=0)\n",
    "\n",
    "    w_star_stacked = jnp.transpose(w_star, (2, 0, 1)).reshape(args.N * R, nw)\n",
    "    z_star_stacked = jnp.transpose(z_star, (2, 0, 1)).reshape(args.N * R, nz)\n",
    "\n",
    "    phi_z_star = args.phi.compute_features(z_star_stacked)\n",
    "    beta_hat = jnp.linalg.solve(\n",
    "        phi_z_star.T @ phi_z_star,\n",
    "        phi_z_star.T @ w_star_stacked\n",
    "    )\n",
    "\n",
    "    # --- Fixed-point iterations ---\n",
    "    def _fixed_point_iteration(_, phi_z):\n",
    "        w_stacked = phi_z @ beta_hat\n",
    "        w = jnp.transpose(w_stacked.reshape(R, args.N, nw), (1, 2, 0))\n",
    "        W = jnp.fft.rfft(w, axis=0)\n",
    "        Z = G_zu @ U + G_zw @ W\n",
    "        z = jnp.fft.irfft(Z, n=args.N, axis=0)\n",
    "        z_stacked = jnp.transpose(z, (2, 0, 1)).reshape(args.N * R, nz)\n",
    "        return args.phi.compute_features(z_stacked)\n",
    "\n",
    "    phi_z = jax.lax.fori_loop(\n",
    "        0, args.fixed_point_iters, _fixed_point_iteration, phi_z_star, unroll=True  # noqa: E501\n",
    "    )\n",
    "\n",
    "    w_hat_stacked = phi_z @ beta_hat\n",
    "    w_hat = jnp.transpose(w_hat_stacked.reshape(R, args.N, nw), (1, 2, 0))\n",
    "    W_beta = jnp.fft.rfft(w_hat, axis=0)\n",
    "\n",
    "    # --- Loss computation ---\n",
    "    Y_hat = G_yu @ U + G_yw @ W_beta\n",
    "    loss_Y = jnp.sqrt(args.Lambda / (R * args.N)) @ (Y - Y_hat)\n",
    "\n",
    "    loss = (loss_Y.real, loss_Y.imag)\n",
    "\n",
    "    MSE_loss = jnp.sum(jnp.abs(loss_Y)**2)\n",
    "    return loss, (MSE_loss, beta_hat)\n",
    "\n",
    "\n",
    "def _prepare_problem(\n",
    "    io_data: InputOutputData,\n",
    "    bla: ModelBLA,\n",
    "    phi: AbstractBasisFunction,\n",
    "    nw: int,\n",
    "    lambda_w: float,\n",
    "    fixed_point_iters: int,\n",
    "    seed: int\n",
    ") -> tuple[ModelNonlinearLFR, dict]:\n",
    "\n",
    "    nz = phi.nz\n",
    "    ny, nx = bla.C_y.shape\n",
    "    N, nu, R, P = io_data.time.u.shape\n",
    "    F = io_data.freq.U.shape[0]\n",
    "\n",
    "    u = io_data.time.u.mean(axis=-1)\n",
    "\n",
    "    # Initialize theta_wz\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    key_B_w, key_C_z, key_D_yw, key_D_zu = jax.random.split(key, 4)\n",
    "\n",
    "    B_w_star = jax.random.normal(key_B_w, (nx, nw))\n",
    "    C_z_star = jax.random.normal(key_C_z, (nz, nx))\n",
    "    D_zu_star = jax.random.normal(key_D_zu, (nz, nu))\n",
    "    D_yw_star = jax.random.normal(key_D_yw, (ny, nw))\n",
    "\n",
    "    theta_wz = _ThetaWZ(B_w_star, C_z_star, D_yw_star, D_zu_star)\n",
    "    theta_uy = (jnp.asarray(bla.A), jnp.asarray(bla.B_u), jnp.asarray(bla.C_y))\n",
    "\n",
    "    # Compute z_star normalization\n",
    "    beta_dummy = np.zeros((phi.num_features(), nw))\n",
    "    f_static_dummy = create_custom_basis_function_model(\n",
    "        nw, phi, beta_dummy\n",
    "    )\n",
    "    nonlin_lfr_dummy = ModelNonlinearLFR(\n",
    "        A=bla.A,\n",
    "        B_u=bla.B_u,\n",
    "        C_y=bla.C_y,\n",
    "        D_yu=bla.D_yu,\n",
    "        B_w=np.zeros_like(B_w_star),\n",
    "        C_z=C_z_star,\n",
    "        D_yw=np.zeros_like(D_yw_star),\n",
    "        D_zu=D_zu_star,\n",
    "        f_static=f_static_dummy,\n",
    "        ts=io_data.time.ts\n",
    "    )\n",
    "    handicap = int(np.ceil(0.25 * N))\n",
    "    z_star = nonlin_lfr_dummy.simulate(u, handicap=handicap)[-1]\n",
    "    z_star_min, z_star_max = z_star.min(axis=(0, 2)), z_star.max(axis=(0, 2))\n",
    "    T_z_inv = jnp.diag(2 / (z_star_max - z_star_min))\n",
    "\n",
    "    # Compute Lambda\n",
    "    Lambda = np.zeros((F, ny, ny))\n",
    "\n",
    "    Y = io_data.freq.Y\n",
    "    Y_P = Y.mean(axis=3)  # Average over periods\n",
    "    if P > 1:\n",
    "        var_noise = ((np.abs(Y - Y_P[..., None])**2).sum(axis=(2, 3))\n",
    "                     / R / (P - 1))\n",
    "        for k in range(F):\n",
    "            np.fill_diagonal(Lambda[k], 1 / var_noise[k])\n",
    "    else:\n",
    "        var_noise = None\n",
    "        for k in range(F):\n",
    "            np.fill_diagonal(Lambda[k], np.eye(ny))\n",
    "            \n",
    "    U_bar = jnp.asarray(io_data.freq.U.mean(axis=3))\n",
    "    Y_bar = jnp.asarray(io_data.freq.Y.mean(axis=3))\n",
    "    f_full = jnp.asarray(io_data.freq.f)\n",
    "    G_yu = jnp.asarray(bla.frequency_response(f_full))\n",
    "    f_data = (f_full, 1 / io_data.time.ts, U_bar, Y_bar, G_yu)\n",
    "\n",
    "    args = _OptiArgs(\n",
    "        theta_uy=theta_uy,\n",
    "        phi=phi,\n",
    "        lambda_w=jnp.asarray(lambda_w, dtype=jnp.float32),\n",
    "        fixed_point_iters=fixed_point_iters,\n",
    "        f_data=f_data,\n",
    "        Lambda=jnp.asarray(Lambda),\n",
    "        Tz_inv=jnp.asarray(T_z_inv),\n",
    "        G_yu=jnp.asarray(bla.frequency_response(io_data.freq.f)),\n",
    "        N=N\n",
    "    )\n",
    "    return theta_wz, args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: inference and learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iterative optimization...\n",
      "   Iteration 0, Loss: 2.2337e+04\n",
      "   Iteration 1, Loss: 2.2337e+04\n",
      "   Iteration 2, Loss: 2.2337e+04\n",
      "   Iteration 3, Loss: 2.2337e+04\n",
      "   Iteration 4, Loss: 2.2337e+04\n",
      "   Iteration 5, Loss: 2.2337e+04\n",
      "   Iteration 6, Loss: 2.2337e+04\n",
      "   Iteration 7, Loss: 2.2337e+04\n",
      "   Iteration 8, Loss: 2.2337e+04\n",
      "   Iteration 9, Loss: 2.2337e+04\n",
      "   Iteration 10, Loss: 2.2337e+04\n",
      "   Iteration 11, Loss: 2.2337e+04\n",
      "   Iteration 12, Loss: 1.9471e+04\n",
      "   Iteration 13, Loss: 1.9471e+04\n",
      "   Iteration 14, Loss: 1.9471e+04\n",
      "   Iteration 15, Loss: 1.9471e+04\n",
      "   Iteration 16, Loss: 1.9471e+04\n",
      "   Iteration 17, Loss: 1.9471e+04\n",
      "   Iteration 18, Loss: 1.9471e+04\n",
      "   Iteration 19, Loss: 1.9471e+04\n",
      "   Iteration 20, Loss: 1.9471e+04\n",
      "   Iteration 21, Loss: 1.9471e+04\n",
      "   Iteration 22, Loss: 1.9471e+04\n",
      "   Iteration 23, Loss: 1.4297e+04\n",
      "   Iteration 24, Loss: 1.4297e+04\n",
      "   Iteration 25, Loss: 1.4297e+04\n",
      "   Iteration 26, Loss: 1.4297e+04\n",
      "   Iteration 27, Loss: 1.4297e+04\n",
      "   Iteration 28, Loss: 1.4297e+04\n",
      "   Iteration 29, Loss: 1.4297e+04\n",
      "   Iteration 30, Loss: 1.4297e+04\n",
      "   Iteration 31, Loss: 1.4297e+04\n",
      "   Iteration 32, Loss: 1.4297e+04\n",
      "   Iteration 33, Loss: 1.4297e+04\n",
      "   Iteration 34, Loss: 1.4297e+04\n",
      "   Iteration 35, Loss: 1.2254e+04\n",
      "   Iteration 36, Loss: 1.2254e+04\n",
      "   Iteration 37, Loss: 1.2254e+04\n",
      "   Iteration 38, Loss: 1.2254e+04\n",
      "   Iteration 39, Loss: 1.2254e+04\n",
      "   Iteration 40, Loss: 1.2254e+04\n",
      "   Iteration 41, Loss: 1.2254e+04\n",
      "   Iteration 42, Loss: 1.2254e+04\n",
      "   Iteration 43, Loss: 1.2254e+04\n",
      "   Iteration 44, Loss: 1.2254e+04\n",
      "   Iteration 45, Loss: 8.4615e+03\n",
      "   Iteration 46, Loss: 8.4615e+03\n",
      "   Iteration 47, Loss: 8.4615e+03\n",
      "   Iteration 48, Loss: 8.4615e+03\n",
      "   Iteration 49, Loss: 8.4615e+03\n",
      "   Iteration 50, Loss: 8.4615e+03\n",
      "   Iteration 51, Loss: 8.4615e+03\n",
      "   Iteration 52, Loss: 8.4615e+03\n",
      "   Iteration 53, Loss: 8.4615e+03\n",
      "   Iteration 54, Loss: 8.4615e+03\n",
      "   Iteration 55, Loss: 8.4615e+03\n",
      "   Iteration 56, Loss: 7.0252e+03\n",
      "   Iteration 57, Loss: 7.0252e+03\n",
      "   Iteration 58, Loss: 7.0252e+03\n",
      "   Iteration 59, Loss: 7.0252e+03\n",
      "   Iteration 60, Loss: 7.0252e+03\n",
      "   Iteration 61, Loss: 7.0252e+03\n",
      "   Iteration 62, Loss: 7.0252e+03\n",
      "   Iteration 63, Loss: 7.0252e+03\n",
      "   Iteration 64, Loss: 7.0252e+03\n",
      "   Iteration 65, Loss: 7.0252e+03\n",
      "   Iteration 66, Loss: 7.0252e+03\n",
      "   Iteration 67, Loss: 6.0147e+03\n",
      "   Iteration 68, Loss: 6.0147e+03\n",
      "   Iteration 69, Loss: 6.0147e+03\n",
      "   Iteration 70, Loss: 6.0147e+03\n",
      "   Iteration 71, Loss: 6.0147e+03\n",
      "   Iteration 72, Loss: 6.0147e+03\n",
      "   Iteration 73, Loss: 6.0147e+03\n",
      "   Iteration 74, Loss: 6.0147e+03\n",
      "   Iteration 75, Loss: 6.0147e+03\n",
      "   Iteration 76, Loss: 6.0147e+03\n",
      "   Iteration 77, Loss: 6.0147e+03\n",
      "   Iteration 78, Loss: 6.0147e+03\n",
      "   Iteration 79, Loss: 5.4480e+03\n",
      "   Iteration 80, Loss: 5.4480e+03\n",
      "   Iteration 81, Loss: 5.4480e+03\n",
      "   Iteration 82, Loss: 5.4480e+03\n",
      "   Iteration 83, Loss: 5.4480e+03\n",
      "   Iteration 84, Loss: 5.4480e+03\n",
      "   Iteration 85, Loss: 5.4480e+03\n",
      "   Iteration 86, Loss: 5.4480e+03\n",
      "   Iteration 87, Loss: 4.3508e+03\n",
      "   Iteration 88, Loss: 4.3508e+03\n",
      "   Iteration 89, Loss: 4.3508e+03\n",
      "   Iteration 90, Loss: 4.3508e+03\n",
      "   Iteration 91, Loss: 4.3508e+03\n",
      "   Iteration 92, Loss: 4.3508e+03\n",
      "   Iteration 93, Loss: 4.3508e+03\n",
      "   Iteration 94, Loss: 4.3508e+03\n",
      "   Iteration 95, Loss: 2.9710e+03\n",
      "   Iteration 96, Loss: 2.9710e+03\n",
      "   Iteration 97, Loss: 2.9710e+03\n",
      "   Iteration 98, Loss: 2.9710e+03\n",
      "   Iteration 99, Loss: 2.9710e+03\n",
      "   Iteration 100, Loss: 2.9710e+03\n",
      "   Iteration 101, Loss: 2.9710e+03\n",
      "   Iteration 102, Loss: 2.9710e+03\n",
      "   Iteration 103, Loss: 2.9710e+03\n",
      "   Iteration 104, Loss: 2.9710e+03\n",
      "   Iteration 105, Loss: 2.6011e+03\n",
      "   Iteration 106, Loss: 2.6011e+03\n",
      "   Iteration 107, Loss: 2.6011e+03\n",
      "   Iteration 108, Loss: 2.6011e+03\n",
      "   Iteration 109, Loss: 2.6011e+03\n",
      "   Iteration 110, Loss: 2.6011e+03\n",
      "   Iteration 111, Loss: 2.6011e+03\n",
      "   Iteration 112, Loss: 2.6011e+03\n",
      "   Iteration 113, Loss: 2.6011e+03\n",
      "   Iteration 114, Loss: 2.6011e+03\n",
      "   Iteration 115, Loss: 2.3579e+03\n",
      "   Iteration 116, Loss: 2.3579e+03\n",
      "   Iteration 117, Loss: 2.3579e+03\n",
      "   Iteration 118, Loss: 2.3579e+03\n",
      "   Iteration 119, Loss: 2.3579e+03\n",
      "   Iteration 120, Loss: 2.3579e+03\n",
      "   Iteration 121, Loss: 2.3579e+03\n",
      "   Iteration 122, Loss: 2.3579e+03\n",
      "   Iteration 123, Loss: 2.3579e+03\n",
      "   Iteration 124, Loss: 2.2814e+03\n",
      "   Iteration 125, Loss: 2.2814e+03\n",
      "   Iteration 126, Loss: 2.2814e+03\n",
      "   Iteration 127, Loss: 2.2814e+03\n",
      "   Iteration 128, Loss: 2.2814e+03\n",
      "   Iteration 129, Loss: 2.2814e+03\n",
      "   Iteration 130, Loss: 2.2814e+03\n",
      "   Iteration 131, Loss: 2.2814e+03\n",
      "   Iteration 132, Loss: 2.2814e+03\n",
      "   Iteration 133, Loss: 2.2814e+03\n",
      "   Iteration 134, Loss: 2.2814e+03\n",
      "   Iteration 135, Loss: 2.2814e+03\n",
      "   Iteration 136, Loss: 2.2814e+03\n",
      "   Iteration 137, Loss: 2.2814e+03\n",
      "   Iteration 138, Loss: 2.2739e+03\n",
      "   Iteration 139, Loss: 2.2739e+03\n",
      "   Iteration 140, Loss: 2.2739e+03\n",
      "   Iteration 141, Loss: 2.2739e+03\n",
      "   Iteration 142, Loss: 2.2739e+03\n",
      "   Iteration 143, Loss: 2.2739e+03\n",
      "   Iteration 144, Loss: 2.2739e+03\n",
      "   Iteration 145, Loss: 2.2739e+03\n",
      "   Iteration 146, Loss: 2.2739e+03\n",
      "   Iteration 147, Loss: 2.2739e+03\n",
      "   Iteration 148, Loss: 2.2739e+03\n",
      "   Iteration 149, Loss: 2.2739e+03\n",
      "   Iteration 150, Loss: 2.2605e+03\n",
      "   Iteration 151, Loss: 2.2605e+03\n",
      "   Iteration 152, Loss: 2.2605e+03\n",
      "   Iteration 153, Loss: 2.2605e+03\n",
      "   Iteration 154, Loss: 2.2605e+03\n",
      "   Iteration 155, Loss: 2.2605e+03\n",
      "   Iteration 156, Loss: 2.2605e+03\n",
      "   Iteration 157, Loss: 2.2605e+03\n",
      "   Iteration 158, Loss: 2.1598e+03\n",
      "   Iteration 159, Loss: 2.1598e+03\n",
      "   Iteration 160, Loss: 2.1598e+03\n",
      "   Iteration 161, Loss: 2.1598e+03\n",
      "   Iteration 162, Loss: 2.1598e+03\n",
      "   Iteration 163, Loss: 2.1598e+03\n",
      "   Iteration 164, Loss: 2.1598e+03\n",
      "   Iteration 165, Loss: 2.1598e+03\n",
      "   Iteration 166, Loss: 2.1598e+03\n",
      "   Iteration 167, Loss: 2.1598e+03\n",
      "   Iteration 168, Loss: 2.1598e+03\n",
      "   Iteration 169, Loss: 2.1598e+03\n",
      "   Iteration 170, Loss: 2.1598e+03\n",
      "   Iteration 171, Loss: 2.1598e+03\n",
      "   Iteration 172, Loss: 2.1598e+03\n",
      "   Iteration 173, Loss: 2.1598e+03\n",
      "   Iteration 174, Loss: 2.1538e+03\n",
      "   Iteration 175, Loss: 2.1538e+03\n",
      "   Iteration 176, Loss: 2.1538e+03\n",
      "   Iteration 177, Loss: 2.1538e+03\n",
      "   Iteration 178, Loss: 2.1538e+03\n",
      "   Iteration 179, Loss: 2.1538e+03\n",
      "   Iteration 180, Loss: 2.1538e+03\n",
      "   Iteration 181, Loss: 2.1538e+03\n",
      "   Iteration 182, Loss: 2.1538e+03\n",
      "   Iteration 183, Loss: 2.1195e+03\n",
      "   Iteration 184, Loss: 2.1195e+03\n",
      "   Iteration 185, Loss: 2.1195e+03\n",
      "   Iteration 186, Loss: 2.1195e+03\n",
      "   Iteration 187, Loss: 2.1195e+03\n",
      "   Iteration 188, Loss: 2.1195e+03\n",
      "   Iteration 189, Loss: 2.1195e+03\n",
      "   Iteration 190, Loss: 2.1195e+03\n",
      "   Iteration 191, Loss: 2.1013e+03\n",
      "   Iteration 192, Loss: 2.1013e+03\n",
      "   Iteration 193, Loss: 2.1013e+03\n",
      "   Iteration 194, Loss: 2.1013e+03\n",
      "   Iteration 195, Loss: 2.1013e+03\n",
      "   Iteration 196, Loss: 2.1013e+03\n",
      "   Iteration 197, Loss: 2.1013e+03\n",
      "   Iteration 198, Loss: 2.1013e+03\n",
      "   Iteration 199, Loss: 2.1013e+03\n",
      "   Iteration 200, Loss: 2.0432e+03\n",
      "   Iteration 201, Loss: 2.0432e+03\n",
      "   Iteration 202, Loss: 2.0432e+03\n",
      "   Iteration 203, Loss: 2.0432e+03\n",
      "   Iteration 204, Loss: 2.0432e+03\n",
      "   Iteration 205, Loss: 2.0432e+03\n",
      "   Iteration 206, Loss: 2.0432e+03\n",
      "   Iteration 207, Loss: 2.0098e+03\n",
      "   Iteration 208, Loss: 2.0098e+03\n",
      "   Iteration 209, Loss: 2.0098e+03\n",
      "   Iteration 210, Loss: 2.0098e+03\n",
      "   Iteration 211, Loss: 1.9601e+03\n",
      "   Iteration 212, Loss: 1.9601e+03\n",
      "   Iteration 213, Loss: 1.9601e+03\n",
      "   Iteration 214, Loss: 1.9601e+03\n",
      "   Iteration 215, Loss: 1.9601e+03\n",
      "   Iteration 216, Loss: 1.8784e+03\n",
      "   Iteration 217, Loss: 1.8784e+03\n",
      "   Iteration 218, Loss: 1.8784e+03\n",
      "   Iteration 219, Loss: 1.8784e+03\n",
      "   Iteration 220, Loss: 1.8784e+03\n",
      "   Iteration 221, Loss: 1.8784e+03\n",
      "   Iteration 222, Loss: 1.8784e+03\n",
      "   Iteration 223, Loss: 1.8784e+03\n",
      "   Iteration 224, Loss: 1.8784e+03\n",
      "   Iteration 225, Loss: 1.7789e+03\n",
      "   Iteration 226, Loss: 1.7789e+03\n",
      "   Iteration 227, Loss: 1.7789e+03\n",
      "   Iteration 228, Loss: 1.7789e+03\n",
      "   Iteration 229, Loss: 1.7789e+03\n",
      "   Iteration 230, Loss: 1.7789e+03\n",
      "   Iteration 231, Loss: 1.7789e+03\n",
      "   Iteration 232, Loss: 1.7789e+03\n",
      "   Iteration 233, Loss: 1.7789e+03\n",
      "   Iteration 234, Loss: 1.7789e+03\n",
      "   Iteration 235, Loss: 1.7789e+03\n",
      "   Iteration 236, Loss: 1.7789e+03\n",
      "   Iteration 237, Loss: 1.7710e+03\n",
      "   Iteration 238, Loss: 1.7710e+03\n",
      "   Iteration 239, Loss: 1.7710e+03\n",
      "   Iteration 240, Loss: 1.7710e+03\n",
      "   Iteration 241, Loss: 1.7710e+03\n",
      "   Iteration 242, Loss: 1.7710e+03\n",
      "   Iteration 243, Loss: 1.7710e+03\n",
      "   Iteration 244, Loss: 1.6285e+03\n",
      "   Iteration 245, Loss: 1.6285e+03\n",
      "   Iteration 246, Loss: 1.6285e+03\n",
      "   Iteration 247, Loss: 1.6285e+03\n",
      "   Iteration 248, Loss: 1.6285e+03\n",
      "   Iteration 249, Loss: 1.6285e+03\n",
      "   Iteration 250, Loss: 1.6285e+03\n",
      "   Iteration 251, Loss: 1.5594e+03\n",
      "   Iteration 252, Loss: 1.5594e+03\n",
      "   Iteration 253, Loss: 1.5594e+03\n",
      "   Iteration 254, Loss: 1.5594e+03\n",
      "   Iteration 255, Loss: 1.5594e+03\n",
      "   Iteration 256, Loss: 1.3971e+03\n",
      "   Iteration 257, Loss: 1.3971e+03\n",
      "   Iteration 258, Loss: 1.3971e+03\n",
      "   Iteration 259, Loss: 1.3971e+03\n",
      "   Iteration 260, Loss: 1.3971e+03\n",
      "   Iteration 261, Loss: 1.3971e+03\n",
      "   Iteration 262, Loss: 1.3971e+03\n",
      "   Iteration 263, Loss: 1.3971e+03\n",
      "   Iteration 264, Loss: 1.3971e+03\n",
      "   Iteration 265, Loss: 1.3795e+03\n",
      "   Iteration 266, Loss: 1.3795e+03\n",
      "   Iteration 267, Loss: 1.3795e+03\n",
      "   Iteration 268, Loss: 1.3795e+03\n",
      "   Iteration 269, Loss: 1.3795e+03\n",
      "   Iteration 270, Loss: 1.3209e+03\n",
      "   Iteration 271, Loss: 1.3209e+03\n",
      "   Iteration 272, Loss: 1.3209e+03\n",
      "   Iteration 273, Loss: 1.3209e+03\n",
      "   Iteration 274, Loss: 1.2347e+03\n",
      "   Iteration 275, Loss: 1.2347e+03\n",
      "   Iteration 276, Loss: 1.2347e+03\n",
      "   Iteration 277, Loss: 1.2347e+03\n",
      "   Iteration 278, Loss: 1.1731e+03\n",
      "   Iteration 279, Loss: 1.1731e+03\n",
      "   Iteration 280, Loss: 1.1731e+03\n",
      "   Iteration 281, Loss: 1.1259e+03\n",
      "   Iteration 282, Loss: 1.1259e+03\n",
      "   Iteration 283, Loss: 1.1259e+03\n",
      "   Iteration 284, Loss: 1.1259e+03\n",
      "   Iteration 285, Loss: 1.1259e+03\n",
      "   Iteration 286, Loss: 1.1259e+03\n",
      "   Iteration 287, Loss: 1.1259e+03\n",
      "   Iteration 288, Loss: 1.1237e+03\n",
      "   Iteration 289, Loss: 1.1237e+03\n",
      "   Iteration 290, Loss: 1.1237e+03\n",
      "   Iteration 291, Loss: 1.1237e+03\n",
      "   Iteration 292, Loss: 1.1237e+03\n",
      "   Iteration 293, Loss: 1.1237e+03\n",
      "   Iteration 294, Loss: 1.1237e+03\n",
      "   Iteration 295, Loss: 1.1103e+03\n",
      "   Iteration 296, Loss: 1.1103e+03\n",
      "   Iteration 297, Loss: 1.1103e+03\n",
      "   Iteration 298, Loss: 1.1103e+03\n",
      "   Iteration 299, Loss: 1.0779e+03\n",
      "   Iteration 300, Loss: 1.0779e+03\n",
      "   Iteration 301, Loss: 1.0779e+03\n",
      "   Iteration 302, Loss: 1.0779e+03\n",
      "   Iteration 303, Loss: 1.0779e+03\n",
      "   Iteration 304, Loss: 1.0779e+03\n",
      "   Iteration 305, Loss: 1.0779e+03\n",
      "   Iteration 306, Loss: 1.0779e+03\n",
      "   Iteration 307, Loss: 1.0779e+03\n",
      "   Iteration 308, Loss: 1.0779e+03\n",
      "   Iteration 309, Loss: 1.0779e+03\n",
      "   Iteration 310, Loss: 1.0779e+03\n",
      "   Iteration 311, Loss: 1.0721e+03\n",
      "   Iteration 312, Loss: 1.0721e+03\n",
      "   Iteration 313, Loss: 1.0721e+03\n",
      "   Iteration 314, Loss: 1.0721e+03\n",
      "   Iteration 315, Loss: 1.0512e+03\n",
      "   Iteration 316, Loss: 1.0512e+03\n",
      "   Iteration 317, Loss: 1.0512e+03\n",
      "   Iteration 318, Loss: 1.0512e+03\n",
      "   Iteration 319, Loss: 1.0512e+03\n",
      "   Iteration 320, Loss: 1.0346e+03\n",
      "   Iteration 321, Loss: 1.0346e+03\n",
      "   Iteration 322, Loss: 1.0346e+03\n",
      "   Iteration 323, Loss: 1.0346e+03\n",
      "   Iteration 324, Loss: 1.0346e+03\n",
      "   Iteration 325, Loss: 9.9865e+02\n",
      "   Iteration 326, Loss: 9.9865e+02\n",
      "   Iteration 327, Loss: 9.8951e+02\n",
      "   Iteration 328, Loss: 9.8951e+02\n",
      "   Iteration 329, Loss: 9.8951e+02\n",
      "   Iteration 330, Loss: 9.6001e+02\n",
      "   Iteration 331, Loss: 9.6001e+02\n",
      "   Iteration 332, Loss: 9.6001e+02\n",
      "   Iteration 333, Loss: 9.6001e+02\n",
      "   Iteration 334, Loss: 9.6001e+02\n",
      "   Iteration 335, Loss: 9.5316e+02\n",
      "   Iteration 336, Loss: 9.5316e+02\n",
      "   Iteration 337, Loss: 9.5316e+02\n",
      "   Iteration 338, Loss: 9.2175e+02\n",
      "   Iteration 339, Loss: 9.2175e+02\n",
      "   Iteration 340, Loss: 9.2175e+02\n",
      "   Iteration 341, Loss: 9.2175e+02\n",
      "   Iteration 342, Loss: 9.1294e+02\n",
      "   Iteration 343, Loss: 9.1294e+02\n",
      "   Iteration 344, Loss: 9.1294e+02\n",
      "   Iteration 345, Loss: 8.8429e+02\n",
      "   Iteration 346, Loss: 8.8429e+02\n",
      "   Iteration 347, Loss: 8.8429e+02\n",
      "   Iteration 348, Loss: 8.8429e+02\n",
      "   Iteration 349, Loss: 8.8429e+02\n",
      "   Iteration 350, Loss: 8.8101e+02\n",
      "   Iteration 351, Loss: 8.8101e+02\n",
      "   Iteration 352, Loss: 8.8101e+02\n",
      "   Iteration 353, Loss: 8.8101e+02\n",
      "   Iteration 354, Loss: 8.8101e+02\n",
      "   Iteration 355, Loss: 8.8101e+02\n",
      "   Iteration 356, Loss: 8.8101e+02\n",
      "   Iteration 357, Loss: 8.8101e+02\n",
      "   Iteration 358, Loss: 8.8101e+02\n",
      "   Iteration 359, Loss: 8.7843e+02\n",
      "   Iteration 360, Loss: 8.7843e+02\n",
      "   Iteration 361, Loss: 8.7843e+02\n",
      "   Iteration 362, Loss: 8.7843e+02\n",
      "   Iteration 363, Loss: 8.6067e+02\n",
      "   Iteration 364, Loss: 8.6067e+02\n",
      "   Iteration 365, Loss: 8.3485e+02\n",
      "   Iteration 366, Loss: 8.3485e+02\n",
      "   Iteration 367, Loss: 8.3485e+02\n",
      "   Iteration 368, Loss: 8.3485e+02\n",
      "   Iteration 369, Loss: 8.2282e+02\n",
      "   Iteration 370, Loss: 8.2282e+02\n",
      "   Iteration 371, Loss: 8.2282e+02\n",
      "   Iteration 372, Loss: 8.0729e+02\n",
      "   Iteration 373, Loss: 8.0729e+02\n",
      "   Iteration 374, Loss: 8.0729e+02\n",
      "   Iteration 375, Loss: 8.0729e+02\n",
      "   Iteration 376, Loss: 8.0095e+02\n",
      "   Iteration 377, Loss: 8.0095e+02\n",
      "   Iteration 378, Loss: 7.8042e+02\n",
      "   Iteration 379, Loss: 7.5479e+02\n",
      "   Iteration 380, Loss: 7.5479e+02\n",
      "   Iteration 381, Loss: 7.5479e+02\n",
      "   Iteration 382, Loss: 7.4812e+02\n",
      "   Iteration 383, Loss: 7.4812e+02\n",
      "   Iteration 384, Loss: 7.4812e+02\n",
      "   Iteration 385, Loss: 7.4812e+02\n",
      "   Iteration 386, Loss: 7.4812e+02\n",
      "   Iteration 387, Loss: 7.4812e+02\n",
      "   Iteration 388, Loss: 7.4812e+02\n",
      "   Iteration 389, Loss: 7.4812e+02\n",
      "   Iteration 390, Loss: 7.4812e+02\n",
      "   Iteration 391, Loss: 7.4812e+02\n",
      "   Iteration 392, Loss: 7.4747e+02\n",
      "   Iteration 393, Loss: 7.4747e+02\n",
      "   Iteration 394, Loss: 7.4747e+02\n",
      "   Iteration 395, Loss: 7.4275e+02\n",
      "   Iteration 396, Loss: 7.4275e+02\n",
      "   Iteration 397, Loss: 7.4275e+02\n",
      "   Iteration 398, Loss: 7.3239e+02\n",
      "   Iteration 399, Loss: 7.3239e+02\n",
      "   Iteration 400, Loss: 7.2994e+02\n",
      "   Iteration 401, Loss: 7.2994e+02\n",
      "   Iteration 402, Loss: 7.2800e+02\n",
      "   Iteration 403, Loss: 7.2800e+02\n",
      "   Iteration 404, Loss: 7.2224e+02\n",
      "   Iteration 405, Loss: 7.1364e+02\n",
      "   Iteration 406, Loss: 7.1364e+02\n",
      "   Iteration 407, Loss: 7.0978e+02\n",
      "   Iteration 408, Loss: 7.0451e+02\n",
      "   Iteration 409, Loss: 7.0451e+02\n",
      "   Iteration 410, Loss: 7.0076e+02\n",
      "   Iteration 411, Loss: 7.0076e+02\n",
      "   Iteration 412, Loss: 7.0076e+02\n",
      "   Iteration 413, Loss: 7.0076e+02\n",
      "   Iteration 414, Loss: 6.9838e+02\n",
      "   Iteration 415, Loss: 6.9838e+02\n",
      "   Iteration 416, Loss: 6.9838e+02\n",
      "   Iteration 417, Loss: 6.9838e+02\n",
      "   Iteration 418, Loss: 6.9612e+02\n",
      "   Iteration 419, Loss: 6.9239e+02\n",
      "   Iteration 420, Loss: 6.9067e+02\n",
      "   Iteration 421, Loss: 6.8499e+02\n",
      "   Iteration 422, Loss: 6.7865e+02\n",
      "   Iteration 423, Loss: 6.7865e+02\n",
      "   Iteration 424, Loss: 6.7412e+02\n",
      "   Iteration 425, Loss: 6.6721e+02\n",
      "   Iteration 426, Loss: 6.6042e+02\n",
      "   Iteration 427, Loss: 6.5566e+02\n",
      "   Iteration 428, Loss: 6.5278e+02\n",
      "   Iteration 429, Loss: 6.4391e+02\n",
      "   Iteration 430, Loss: 6.4391e+02\n",
      "   Iteration 431, Loss: 6.4391e+02\n",
      "   Iteration 432, Loss: 6.4391e+02\n",
      "   Iteration 433, Loss: 6.4222e+02\n",
      "   Iteration 434, Loss: 6.4222e+02\n",
      "   Iteration 435, Loss: 6.4222e+02\n",
      "   Iteration 436, Loss: 6.4222e+02\n",
      "   Iteration 437, Loss: 6.4222e+02\n",
      "   Iteration 438, Loss: 6.4092e+02\n",
      "   Iteration 439, Loss: 6.4092e+02\n",
      "   Iteration 440, Loss: 6.3704e+02\n",
      "   Iteration 441, Loss: 6.3704e+02\n",
      "   Iteration 442, Loss: 6.3704e+02\n",
      "   Iteration 443, Loss: 6.3508e+02\n",
      "   Iteration 444, Loss: 6.3508e+02\n",
      "   Iteration 445, Loss: 6.3508e+02\n",
      "   Iteration 446, Loss: 6.3508e+02\n",
      "   Iteration 447, Loss: 6.3477e+02\n",
      "   Iteration 448, Loss: 6.3124e+02\n",
      "   Iteration 449, Loss: 6.2651e+02\n",
      "   Iteration 450, Loss: 6.2651e+02\n",
      "   Iteration 451, Loss: 6.2651e+02\n",
      "   Iteration 452, Loss: 6.2401e+02\n",
      "   Iteration 453, Loss: 6.2401e+02\n",
      "   Iteration 454, Loss: 6.2144e+02\n",
      "   Iteration 455, Loss: 6.2144e+02\n",
      "   Iteration 456, Loss: 6.2144e+02\n",
      "   Iteration 457, Loss: 6.1692e+02\n",
      "   Iteration 458, Loss: 6.1692e+02\n",
      "   Iteration 459, Loss: 6.1692e+02\n",
      "   Iteration 460, Loss: 6.1692e+02\n",
      "   Iteration 461, Loss: 6.1692e+02\n",
      "   Iteration 462, Loss: 6.1692e+02\n",
      "   Iteration 463, Loss: 6.1692e+02\n",
      "   Iteration 464, Loss: 6.1652e+02\n",
      "   Iteration 465, Loss: 6.1652e+02\n",
      "   Iteration 466, Loss: 6.1471e+02\n",
      "   Iteration 467, Loss: 6.1471e+02\n",
      "   Iteration 468, Loss: 6.1471e+02\n",
      "   Iteration 469, Loss: 6.1471e+02\n",
      "   Iteration 470, Loss: 6.1334e+02\n",
      "   Iteration 471, Loss: 6.1334e+02\n",
      "   Iteration 472, Loss: 6.1083e+02\n",
      "   Iteration 473, Loss: 6.1083e+02\n",
      "   Iteration 474, Loss: 6.0466e+02\n",
      "   Iteration 475, Loss: 6.0263e+02\n",
      "   Iteration 476, Loss: 6.0263e+02\n",
      "   Iteration 477, Loss: 5.9905e+02\n",
      "   Iteration 478, Loss: 5.9905e+02\n",
      "   Iteration 479, Loss: 5.9905e+02\n",
      "   Iteration 480, Loss: 5.9534e+02\n",
      "   Iteration 481, Loss: 5.9534e+02\n",
      "   Iteration 482, Loss: 5.9534e+02\n",
      "   Iteration 483, Loss: 5.9230e+02\n",
      "   Iteration 484, Loss: 5.9230e+02\n",
      "   Iteration 485, Loss: 5.9230e+02\n",
      "   Iteration 486, Loss: 5.9116e+02\n",
      "   Iteration 487, Loss: 5.9116e+02\n",
      "   Iteration 488, Loss: 5.9001e+02\n",
      "   Iteration 489, Loss: 5.8652e+02\n",
      "   Iteration 490, Loss: 5.8564e+02\n",
      "   Iteration 491, Loss: 5.8564e+02\n",
      "   Iteration 492, Loss: 5.8564e+02\n",
      "   Iteration 493, Loss: 5.8448e+02\n",
      "   Iteration 494, Loss: 5.8448e+02\n",
      "   Iteration 495, Loss: 5.8282e+02\n",
      "   Iteration 496, Loss: 5.8282e+02\n",
      "   Iteration 497, Loss: 5.8221e+02\n",
      "   Iteration 498, Loss: 5.8110e+02\n",
      "   Iteration 499, Loss: 5.8110e+02\n",
      "   Iteration 500, Loss: 5.8003e+02\n",
      "   Iteration 501, Loss: 5.8003e+02\n",
      "   Iteration 502, Loss: 5.7894e+02\n",
      "   Iteration 503, Loss: 5.7894e+02\n",
      "   Iteration 504, Loss: 5.7695e+02\n",
      "   Iteration 505, Loss: 5.7695e+02\n",
      "   Iteration 506, Loss: 5.7592e+02\n",
      "   Iteration 507, Loss: 5.7592e+02\n",
      "   Iteration 508, Loss: 5.7592e+02\n",
      "   Iteration 509, Loss: 5.7400e+02\n",
      "   Iteration 510, Loss: 5.7400e+02\n",
      "   Iteration 511, Loss: 5.7274e+02\n",
      "   Iteration 512, Loss: 5.7274e+02\n",
      "   Iteration 513, Loss: 5.7193e+02\n",
      "   Iteration 514, Loss: 5.7193e+02\n",
      "   Iteration 515, Loss: 5.7193e+02\n",
      "   Iteration 516, Loss: 5.7138e+02\n",
      "   Iteration 517, Loss: 5.7013e+02\n",
      "   Iteration 518, Loss: 5.6967e+02\n",
      "   Iteration 519, Loss: 5.6967e+02\n",
      "   Iteration 520, Loss: 5.6967e+02\n",
      "   Iteration 521, Loss: 5.6879e+02\n",
      "   Iteration 522, Loss: 5.6794e+02\n",
      "   Iteration 523, Loss: 5.6719e+02\n",
      "   Iteration 524, Loss: 5.6636e+02\n",
      "   Iteration 525, Loss: 5.6636e+02\n",
      "   Iteration 526, Loss: 5.6636e+02\n",
      "   Iteration 527, Loss: 5.6595e+02\n",
      "   Iteration 528, Loss: 5.6534e+02\n",
      "   Iteration 529, Loss: 5.6444e+02\n",
      "   Iteration 530, Loss: 5.6444e+02\n",
      "   Iteration 531, Loss: 5.6376e+02\n",
      "   Iteration 532, Loss: 5.6340e+02\n",
      "   Iteration 533, Loss: 5.6221e+02\n",
      "   Iteration 534, Loss: 5.6221e+02\n",
      "   Iteration 535, Loss: 5.6198e+02\n",
      "   Iteration 536, Loss: 5.6078e+02\n",
      "   Iteration 537, Loss: 5.6009e+02\n",
      "   Iteration 538, Loss: 5.5837e+02\n",
      "   Iteration 539, Loss: 5.5837e+02\n",
      "   Iteration 540, Loss: 5.5837e+02\n",
      "   Iteration 541, Loss: 5.5837e+02\n",
      "   Iteration 542, Loss: 5.5837e+02\n",
      "   Iteration 543, Loss: 5.5795e+02\n",
      "   Iteration 544, Loss: 5.5795e+02\n",
      "   Iteration 545, Loss: 5.5730e+02\n",
      "   Iteration 546, Loss: 5.5673e+02\n",
      "   Iteration 547, Loss: 5.5673e+02\n",
      "   Iteration 548, Loss: 5.5545e+02\n",
      "   Iteration 549, Loss: 5.5381e+02\n",
      "   Iteration 550, Loss: 5.5302e+02\n",
      "   Iteration 551, Loss: 5.5302e+02\n",
      "   Iteration 552, Loss: 5.5255e+02\n",
      "   Iteration 553, Loss: 5.5255e+02\n",
      "   Iteration 554, Loss: 5.5255e+02\n",
      "   Iteration 555, Loss: 5.5180e+02\n",
      "   Iteration 556, Loss: 5.4971e+02\n",
      "   Iteration 557, Loss: 5.4927e+02\n",
      "   Iteration 558, Loss: 5.4791e+02\n",
      "   Iteration 559, Loss: 5.4791e+02\n",
      "   Iteration 560, Loss: 5.4658e+02\n",
      "   Iteration 561, Loss: 5.4529e+02\n",
      "   Iteration 562, Loss: 5.4422e+02\n",
      "   Iteration 563, Loss: 5.4349e+02\n",
      "   Iteration 564, Loss: 5.4218e+02\n",
      "   Iteration 565, Loss: 5.4123e+02\n",
      "   Iteration 566, Loss: 5.4123e+02\n",
      "   Iteration 567, Loss: 5.4002e+02\n",
      "   Iteration 568, Loss: 5.4002e+02\n",
      "   Iteration 569, Loss: 5.4002e+02\n",
      "   Iteration 570, Loss: 5.3946e+02\n",
      "   Iteration 571, Loss: 5.3946e+02\n",
      "   Iteration 572, Loss: 5.3869e+02\n",
      "   Iteration 573, Loss: 5.3869e+02\n",
      "   Iteration 574, Loss: 5.3869e+02\n",
      "   Iteration 575, Loss: 5.3869e+02\n",
      "   Iteration 576, Loss: 5.3869e+02\n",
      "   Iteration 577, Loss: 5.3857e+02\n",
      "   Iteration 578, Loss: 5.3857e+02\n",
      "   Iteration 579, Loss: 5.3825e+02\n",
      "   Iteration 580, Loss: 5.3825e+02\n",
      "   Iteration 581, Loss: 5.3784e+02\n",
      "   Iteration 582, Loss: 5.3784e+02\n",
      "   Iteration 583, Loss: 5.3784e+02\n",
      "   Iteration 584, Loss: 5.3733e+02\n",
      "   Iteration 585, Loss: 5.3733e+02\n",
      "   Iteration 586, Loss: 5.3712e+02\n",
      "   Iteration 587, Loss: 5.3660e+02\n",
      "   Iteration 588, Loss: 5.3660e+02\n",
      "   Iteration 589, Loss: 5.3626e+02\n",
      "   Iteration 590, Loss: 5.3626e+02\n",
      "   Iteration 591, Loss: 5.3605e+02\n",
      "   Iteration 592, Loss: 5.3581e+02\n",
      "   Iteration 593, Loss: 5.3581e+02\n",
      "   Iteration 594, Loss: 5.3556e+02\n",
      "   Iteration 595, Loss: 5.3545e+02\n",
      "   Iteration 596, Loss: 5.3508e+02\n",
      "   Iteration 597, Loss: 5.3508e+02\n",
      "   Iteration 598, Loss: 5.3492e+02\n",
      "   Iteration 599, Loss: 5.3483e+02\n",
      "   Iteration 600, Loss: 5.3441e+02\n",
      "   Iteration 601, Loss: 5.3424e+02\n",
      "   Iteration 602, Loss: 5.3399e+02\n",
      "   Iteration 603, Loss: 5.3370e+02\n",
      "   Iteration 604, Loss: 5.3340e+02\n",
      "   Iteration 605, Loss: 5.3305e+02\n",
      "   Iteration 606, Loss: 5.3296e+02\n",
      "   Iteration 607, Loss: 5.3283e+02\n",
      "   Iteration 608, Loss: 5.3247e+02\n",
      "   Iteration 609, Loss: 5.3228e+02\n",
      "   Iteration 610, Loss: 5.3204e+02\n",
      "   Iteration 611, Loss: 5.3135e+02\n",
      "   Iteration 612, Loss: 5.3052e+02\n",
      "   Iteration 613, Loss: 5.3052e+02\n",
      "   Iteration 614, Loss: 5.3023e+02\n",
      "   Iteration 615, Loss: 5.2930e+02\n",
      "   Iteration 616, Loss: 5.2930e+02\n",
      "   Iteration 617, Loss: 5.2881e+02\n",
      "   Iteration 618, Loss: 5.2881e+02\n",
      "   Iteration 619, Loss: 5.2881e+02\n",
      "   Iteration 620, Loss: 5.2881e+02\n",
      "   Iteration 621, Loss: 5.2881e+02\n",
      "   Iteration 622, Loss: 5.2864e+02\n",
      "   Iteration 623, Loss: 5.2829e+02\n",
      "   Iteration 624, Loss: 5.2829e+02\n",
      "   Iteration 625, Loss: 5.2796e+02\n",
      "   Iteration 626, Loss: 5.2796e+02\n",
      "   Iteration 627, Loss: 5.2774e+02\n",
      "   Iteration 628, Loss: 5.2760e+02\n",
      "   Iteration 629, Loss: 5.2720e+02\n",
      "   Iteration 630, Loss: 5.2720e+02\n",
      "   Iteration 631, Loss: 5.2676e+02\n",
      "   Iteration 632, Loss: 5.2676e+02\n",
      "   Iteration 633, Loss: 5.2609e+02\n",
      "   Iteration 634, Loss: 5.2490e+02\n",
      "   Iteration 635, Loss: 5.2490e+02\n",
      "   Iteration 636, Loss: 5.2476e+02\n",
      "   Iteration 637, Loss: 5.2476e+02\n",
      "   Iteration 638, Loss: 5.2368e+02\n",
      "   Iteration 639, Loss: 5.2368e+02\n",
      "   Iteration 640, Loss: 5.2285e+02\n",
      "   Iteration 641, Loss: 5.2223e+02\n",
      "   Iteration 642, Loss: 5.2122e+02\n",
      "   Iteration 643, Loss: 5.2028e+02\n",
      "   Iteration 644, Loss: 5.2028e+02\n",
      "   Iteration 645, Loss: 5.1985e+02\n",
      "   Iteration 646, Loss: 5.1910e+02\n",
      "   Iteration 647, Loss: 5.1910e+02\n",
      "   Iteration 648, Loss: 5.1879e+02\n",
      "   Iteration 649, Loss: 5.1856e+02\n",
      "   Iteration 650, Loss: 5.1766e+02\n",
      "   Iteration 651, Loss: 5.1753e+02\n",
      "   Iteration 652, Loss: 5.1690e+02\n",
      "   Iteration 653, Loss: 5.1662e+02\n",
      "   Iteration 654, Loss: 5.1577e+02\n",
      "   Iteration 655, Loss: 5.1483e+02\n",
      "   Iteration 656, Loss: 5.1392e+02\n",
      "   Iteration 657, Loss: 5.1392e+02\n",
      "   Iteration 658, Loss: 5.1315e+02\n",
      "   Iteration 659, Loss: 5.1244e+02\n",
      "   Iteration 660, Loss: 5.1160e+02\n",
      "   Iteration 661, Loss: 5.1160e+02\n",
      "   Iteration 662, Loss: 5.1160e+02\n",
      "   Iteration 663, Loss: 5.1093e+02\n",
      "   Iteration 664, Loss: 5.0972e+02\n",
      "   Iteration 665, Loss: 5.0972e+02\n",
      "   Iteration 666, Loss: 5.0881e+02\n",
      "   Iteration 667, Loss: 5.0881e+02\n",
      "   Iteration 668, Loss: 5.0839e+02\n",
      "   Iteration 669, Loss: 5.0783e+02\n",
      "   Iteration 670, Loss: 5.0783e+02\n",
      "   Iteration 671, Loss: 5.0752e+02\n",
      "   Iteration 672, Loss: 5.0728e+02\n",
      "   Iteration 673, Loss: 5.0728e+02\n",
      "   Iteration 674, Loss: 5.0681e+02\n",
      "   Iteration 675, Loss: 5.0681e+02\n",
      "   Iteration 676, Loss: 5.0635e+02\n",
      "   Iteration 677, Loss: 5.0570e+02\n",
      "   Iteration 678, Loss: 5.0490e+02\n",
      "   Iteration 679, Loss: 5.0490e+02\n",
      "   Iteration 680, Loss: 5.0461e+02\n",
      "   Iteration 681, Loss: 5.0461e+02\n",
      "   Iteration 682, Loss: 5.0358e+02\n",
      "   Iteration 683, Loss: 5.0279e+02\n",
      "   Iteration 684, Loss: 5.0279e+02\n",
      "   Iteration 685, Loss: 5.0279e+02\n",
      "   Iteration 686, Loss: 5.0240e+02\n",
      "   Iteration 687, Loss: 5.0240e+02\n",
      "   Iteration 688, Loss: 5.0174e+02\n",
      "   Iteration 689, Loss: 5.0162e+02\n",
      "   Iteration 690, Loss: 5.0162e+02\n",
      "   Iteration 691, Loss: 5.0162e+02\n",
      "   Iteration 692, Loss: 5.0136e+02\n",
      "   Iteration 693, Loss: 5.0099e+02\n",
      "   Iteration 694, Loss: 5.0041e+02\n",
      "   Iteration 695, Loss: 5.0006e+02\n",
      "   Iteration 696, Loss: 4.9976e+02\n",
      "   Iteration 697, Loss: 4.9945e+02\n",
      "   Iteration 698, Loss: 4.9907e+02\n",
      "   Iteration 699, Loss: 4.9852e+02\n",
      "   Iteration 700, Loss: 4.9852e+02\n",
      "   Iteration 701, Loss: 4.9809e+02\n",
      "   Iteration 702, Loss: 4.9767e+02\n",
      "   Iteration 703, Loss: 4.9729e+02\n",
      "   Iteration 704, Loss: 4.9713e+02\n",
      "   Iteration 705, Loss: 4.9639e+02\n",
      "   Iteration 706, Loss: 4.9593e+02\n",
      "   Iteration 707, Loss: 4.9540e+02\n",
      "   Iteration 708, Loss: 4.9479e+02\n",
      "   Iteration 709, Loss: 4.9479e+02\n",
      "   Iteration 710, Loss: 4.9437e+02\n",
      "   Iteration 711, Loss: 4.9437e+02\n",
      "   Iteration 712, Loss: 4.9415e+02\n",
      "   Iteration 713, Loss: 4.9415e+02\n",
      "   Iteration 714, Loss: 4.9395e+02\n",
      "   Iteration 715, Loss: 4.9369e+02\n",
      "   Iteration 716, Loss: 4.9353e+02\n",
      "   Iteration 717, Loss: 4.9331e+02\n",
      "   Iteration 718, Loss: 4.9331e+02\n",
      "   Iteration 719, Loss: 4.9322e+02\n",
      "   Iteration 720, Loss: 4.9322e+02\n",
      "   Iteration 721, Loss: 4.9320e+02\n",
      "   Iteration 722, Loss: 4.9303e+02\n",
      "   Iteration 723, Loss: 4.9303e+02\n",
      "   Iteration 724, Loss: 4.9303e+02\n",
      "   Iteration 725, Loss: 4.9303e+02\n",
      "   Iteration 726, Loss: 4.9295e+02\n",
      "   Iteration 727, Loss: 4.9295e+02\n",
      "   Iteration 728, Loss: 4.9288e+02\n",
      "   Iteration 729, Loss: 4.9288e+02\n",
      "   Iteration 730, Loss: 4.9288e+02\n",
      "   Iteration 731, Loss: 4.9288e+02\n",
      "   Iteration 732, Loss: 4.9288e+02\n",
      "   Iteration 733, Loss: 4.9288e+02\n",
      "   Iteration 734, Loss: 4.9288e+02\n",
      "   Iteration 735, Loss: 4.9288e+02\n",
      "   Iteration 736, Loss: 4.9288e+02\n",
      "   Iteration 737, Loss: 4.9288e+02\n",
      "   Iteration 738, Loss: 4.9287e+02\n",
      "\n",
      "\n",
      "NRMSE of nonlin_lfr: 1.68%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the nonlinear basis function\n",
    "polynomial_degree = 7\n",
    "nw = 2\n",
    "nz = 2\n",
    "phi = basis_functions.Polynomial(nz, polynomial_degree)\n",
    "\n",
    "import optax \n",
    "\n",
    "solver = optx.OptaxMinimiser(optax.adam(learning_rate=1e-3), rtol=1e-3, atol=1e-6)\n",
    "solver = optx.BFGS(rtol=1e-3, atol=1e-6)\n",
    "\n",
    "\n",
    "# Define inference and learning hyperparameters\n",
    "lambda_w = 1\n",
    "fixed_point_iterations = 5\n",
    "\n",
    "# Solve the problem\n",
    "nonlin_lfr = solverrr(io_data, bla_opti, phi, nw, lambda_w, fixed_point_iterations, solver, 10000, seed)\n",
    "\n",
    "# Simulate and check time-domain performance\n",
    "y_sim_nonlin_lfr = nonlin_lfr.simulate(u_bar, handicap=handicap)[0]\n",
    "NRMSE_nonlin_lfr = 100 * np.sqrt(np.mean((y_bar - y_sim_nonlin_lfr)**2)) / np.sqrt(np.mean(y_bar**2)) \n",
    "\n",
    "print(f'NRMSE of nonlin_lfr: {NRMSE_nonlin_lfr:.2f}%\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.1222291e-04 -2.4430049e-04]\n",
      " [-9.1545859e-05 -1.2276375e-04]\n",
      " [ 7.2191661e-04 -1.1256315e-03]\n",
      " [-1.3463055e-04 -2.7200184e-04]\n",
      " [ 4.3029609e-04  1.3532129e-04]\n",
      " [-1.9501754e-03  3.2395015e-03]\n",
      " [-5.0938519e-04  7.8019591e-05]\n",
      " [-4.4485510e-05 -5.3317507e-04]\n",
      " [ 5.7714613e-04 -2.1349544e-04]\n",
      " [-1.5150787e-03  2.0597121e-03]\n",
      " [ 9.7047101e-05 -1.1007484e-03]\n",
      " [-2.4303029e-06  2.7526354e-05]\n",
      " [ 4.6904184e-04  1.1575153e-04]\n",
      " [-6.5723871e-04  4.5630496e-04]\n",
      " [ 1.7549624e-03 -3.3680950e-03]\n",
      " [ 1.3637821e-03  1.5401845e-03]\n",
      " [-8.9733355e-04  7.8945112e-04]\n",
      " [ 5.9545587e-04 -3.4795579e-05]\n",
      " [ 1.5972934e-04 -4.5064121e-04]\n",
      " [-1.1526899e-04  2.1643669e-04]\n",
      " [-9.9257415e-04  2.5774606e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(nonlin_lfr.f_static.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nonlinear_lfr_opti' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Optimise one-step-ahead state predictions (optional)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m ModelNonlinearLFR_opti = \u001b[43mnonlinear_lfr_opti\u001b[49m.optimise_state_predictions(\n\u001b[32m      3\u001b[39m     IDM,\n\u001b[32m      4\u001b[39m     solver=optx.BFGS(rtol=\u001b[32m1e-8\u001b[39m, atol=\u001b[32m1e-8\u001b[39m),\n\u001b[32m      5\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'nonlinear_lfr_opti' is not defined"
     ]
    }
   ],
   "source": [
    "# Optimise one-step-ahead state predictions (optional)\n",
    "ModelNonlinearLFR_opti = nonlinear_lfr_opti.optimise_state_predictions(\n",
    "    IDM,\n",
    "    solver=optx.BFGS(rtol=1e-8, atol=1e-8),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optimise one-step-ahead state predictions (optional)\n",
    "# ModelNonlinearLFR_opti = nonlinear_lfr_opti.optimise_state_predictions(\n",
    "#     IDM,\n",
    "#     solver=optx.BFGS(rtol=1e-8, atol=1e-8),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimise simulation error# Optimise one-step-ahead state predictions (optional)\n",
    "# ModelNonlinearLFR_opti = nonlinear_lfr_opti.optimise_state_predictions(\n",
    "#     IDM,\n",
    "#     solver=optx.BFGS(rtol=1e-8, atol=1e-8),\n",
    "# )\n",
    "\n",
    "from reinbos.utils import misc\n",
    "\n",
    "custom_init = nonlinear_lfr_opti.DecisionVars(\n",
    "    B_w=misc.OptiParam(1/10*ModelNonlinearLFR.B_w),\n",
    "    D_yw=misc.OptiParam(1/10*ModelNonlinearLFR.D_yw),\n",
    ")\n",
    "\n",
    "\n",
    "ModelNonlinearLFR_opti_sim = nonlinear_lfr_opti.optimise_simulation_error(\n",
    "    IDM,\n",
    "    solver=optx.LevenbergMarquardt(rtol=1e-3, atol=1e-6),\n",
    "    max_iter=200,\n",
    "    custom_init=custom_init,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "ParWH_test = [\n",
    "    data for data in ParWH_full_test\n",
    "    if data.name == f'Val-amp-{amp_level}' or data.name == 'ValArr'\n",
    "]\n",
    "\n",
    "test_ms = ParWH_test[0]\n",
    "test_arr = ParWH_test[2]\n",
    "\n",
    "# Multisine test\n",
    "u_test_ms = np.transpose(test_ms.u.reshape(1, nu, N, 2), (2, 1, 0, 3))\n",
    "y_test_ms = test_ms.y[::2]\n",
    "u_test_ms = (u_test_ms - u_mean) / u_std\n",
    "\n",
    "y_bla_ms = IDM.bla.opti.model.simulate(u_test_ms)[1]\n",
    "y_lfr_ms = ModelNonlinearLFR_opti_sim.simulate(u_test_ms)[2]\n",
    "y_bla_ms = np.squeeze(y_bla_ms[..., 1] * y_std + y_mean)\n",
    "y_lfr_ms = np.squeeze(y_lfr_ms[..., 1] * y_std + y_mean)\n",
    "\n",
    "e_bla_ms = y_test_ms - y_bla_ms\n",
    "e_lfr_ms = y_test_ms - y_lfr_ms\n",
    "\n",
    "E_bla_ms = 1 / N * np.fft.rfft(e_bla_ms, axis=0)\n",
    "E_lfr_ms = 1 / N * np.fft.rfft(e_lfr_ms, axis=0)\n",
    "Y_test_ms = 1 / N * np.fft.rfft(y_test_ms, axis=0)\n",
    "\n",
    "print(f'Multisine test RMSE BLA: {np.sqrt(np.mean(e_bla_ms**2)):.4e} ({100*np.std(e_bla_ms)/np.std(y_test_ms):.2f}%)')\n",
    "print(f'Multisine test RMSE nonlinear LFR: {np.sqrt(np.mean(e_lfr_ms**2)):.4e} ({100*np.std(e_lfr_ms)/np.std(y_test_ms):.2f}%)')\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axs[0].plot(IDM.data.time.t, y_test_ms, label='system output')\n",
    "axs[0].plot(IDM.data.time.t, e_bla_ms, label='BLA error')\n",
    "axs[0].plot(IDM.data.time.t, e_lfr_ms, label='nonlinear LFR error')\n",
    "axs[0].set_title('Multisine - Time Domain')\n",
    "axs[0].set_xlabel('time [s]')\n",
    "axs[0].set_ylabel('amplitude [-]')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(IDM.data.freq.f[f_idx], 20*np.log10(np.abs(Y_test_ms[f_idx])), label='system output')\n",
    "axs[1].plot(IDM.data.freq.f[f_idx], 20*np.log10(np.abs(E_bla_ms[f_idx])), label='BLA error')\n",
    "axs[1].plot(IDM.data.freq.f[f_idx], 20*np.log10(np.abs(E_lfr_ms[f_idx])), label='nonlinear LFR error')\n",
    "axs[1].set_title('Multisine - Frequency Domain')\n",
    "axs[1].set_xlabel('frequency [Hz]')\n",
    "axs[1].set_ylabel('magnitude [dB]')\n",
    "axs[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Arrow test\n",
    "u_test_arr = test_arr.u.reshape(-1, 1, 1)\n",
    "u_test_arr = (u_test_arr - u_mean) / u_std\n",
    "y_test_arr = test_arr.y\n",
    "\n",
    "y_bla_ss = IDM.bla.opti.model.simulate(u_test_arr, P_trans=1)[1]\n",
    "y_lfr_ss = ModelNonlinearLFR_opti_sim.simulate(u_test_arr, P_trans=1)[2]\n",
    "y_bla_ss = np.squeeze(y_bla_ss * y_std + y_mean)\n",
    "y_lfr_ss = np.squeeze(y_lfr_ss * y_std + y_mean)\n",
    "\n",
    "e_bla_ss = y_test_arr - y_bla_ss\n",
    "e_lfr_ss = y_test_arr - y_lfr_ss\n",
    "\n",
    "print(f'Arrow test RMSE BLA: {np.sqrt(np.mean(e_bla_ss**2)):.4e} ({100*np.std(e_bla_ss)/np.std(y_test_arr):.2f}%)')\n",
    "print(f'Arrow test RMSE nonlinear LFR: {np.sqrt(np.mean(e_lfr_ss**2)):.4e} ({100*np.std(e_lfr_ss)/np.std(y_test_arr):.2f}%)')\n",
    "\n",
    "t_ss = np.linspace(0, len(y_test_arr) / fs, len(y_test_arr))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(t_ss, y_test_arr, label='system output')\n",
    "plt.plot(t_ss, e_bla_ss, label='BLA error')\n",
    "plt.plot(t_ss, e_lfr_ss, label='nonlinear LFR error')\n",
    "plt.title('Arrow test - Time Domain')\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('amplitude [-]')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
